---
title: "A geo-referenced micro-data set of real estate listings for Spain’s three largest cities - Response to referees"
output: pdf_document
bibliography: bibEPB.bib
link-citations: yes
linkcolor: blue
---

```{r load-packages, include=FALSE}
library(dplyr)
library(ggplot2)
library(glue)
library(gridExtra)
library(idealista18)
library(kableExtra)
library(sf)
```

<!--
```{r install-data-package, include = FALSE}
# Run only once if needed to install data package `sobiEquity`
if (!require("idealista18", character.only = TRUE)) {
devtools::install_github("https://github.com/paezha/idealista18")
}
```

```{r setup,  cache=FALSE, warning = FALSE, include=FALSE, message = FALSE}
library(idealista18)
library(tidyverse)

set.seed(1234)
```
-->

Referee: 1

>Comments to the Author  

>Dear Authors, Thank you for the opportunity to review this paper. I would appreciate it if you would highlight the changed text in the next submission. Otherwise it is difficult to identify which parts have changed. The unknown quality of this one year randomised data is the main issue for potential users and I repeat my suggestion that this data should be validated for the benefit potential readers and users. Below is a list of comments and questions which  need to be addressed.

Thank you for your constructive comments. Below we describe how we improved the paper in response.

---

>1. Please clearly state the nature of the data in the abstract. It is worthwhile to mention the relocated and randomised dataset in the abstract. This house price dataset only has 189,923 records, which could be not really be described as a big dataset as you describe it in the abstract. Except the authors could provide an evidence/reference on why this is big dataset.

**Response:** 

We appreciate this comment. There are a few publicly available datasets for hedonic price analysis, but they tend to be smaller. For example, @bonifaci2015real includes $n=1,042$ observations for Padua, in Italy; @delgiudice2018housing share a dataset with $n=576$ observations relating to rental prices in Naples, Italy; @solano2019dataset present a dataset with $n=1,623$ daily rental prices in Seville, Spain; and the dataset of @fuerst2020real includes $n=4,201$ property prices geocoded to the level of nine regions in England and Wales. The most similar in terms of geographical disaggregation and sample size is the dataset of @song2021hedonic which includes transactions for four cities in South Korea, namely Busan ($n=61,152$), Daegu ($n=32,363$), Daejeon ($n=21,114$), and Gwangju ($n=25,984$). In response to your comment we drop the terminology of "big data" and describe our dataset as "large" instead.  

---

>2. Please check the typographical errors before your next submission. For example, the “thirdr” in your table1.

**Response:** 

Thank you for highlighting this. We have carefully proof-read the document for typos and other errors.

---

>3. Please use part of your answer to my previous fourth question in the manuscript. Your answer is quite useful for your future users, especially those researchers who are not familiar with house price data in Spain. I quote your previous answer below:

>“idealista covers fairly well all segments in the Spanish market, both individual and professional advertisers. This dataset comprehends information about listing prices, thus it contains the market situation from the offer. Nevertheless, actual sale prices information is not publicly available as it is information that can only be accessed by paying high fees to Colegio de Registradores. However listing prices reflect quite well the (transaction) reality of real estate markets using the offer perspective, they keep strong correlation between idealista and transaction prices can be established see Banco de España. Even though a great extent of correlation does exist between official transactional and asking records, both databases can be taken as complementary and are of great interest when studying asking-transaction price gaps or the relation between listing site demand variables (ie. ad contacts or ad views) and price gaps. The main real estate portals in Spain are idealista and Fotocasa, further away are Habitaclia and Milanuncios, the latter focusing almost exclusively on private individuals. In September 2021, according to data from Similarweb (a site specialized in sites’ traffic volume comparison), in Spain, there were a total of 103 million page views on real estate portals, out of page views on real estate portals, of which the four main portals, idealista, Fotocasa, Habitaclia, and Pisos.com, in that order, accounted for 94% of the traffic. Another relevant feature of this sector is that it is highly concentrated, idealista being the leader by far with 58.6 monthly million visits (57% of the total traffic) versus its immediate competitor with 19.9 million visits (19.3 % of total traffic). In terms of the evolution over time of interest in the content of each portal, based on data from Google Trends, the leadership of idealista Trends, the leadership of idealist”

**Response:**

Thank you for this suggestion. We have adapted this answer to expand the text in the section "Data Description".

---

>4. For data description, it is worth showing solid evidence of the quality of this dataset. For example, what is the data coverage of this data compared to all the residential properties in the city?  Listing the advertised volume is not very helpful for users who wish to understand the quality.

**Response:**

To better place in context the coverage of the {idealista18} dataset, Table \ref{tab:transactions} now shows the number of listings with respect to the total residential stock in each city (in 2018). As seen in the table, the number of listings ranges between 6.1% of the total number of properties (in Madrid) and 8.1% (in Valencia). Information from Instituto Nacional de Estadistica^[\url{https://www.ine.es/jaxiT3/Tabla.htm?t=6150&L=1}] for 2018 also shows that the number of transactions in 2018 are equivalent to 81.3\% of idealista listings in Barcelona, 80.8\% in Madrid, and 91.1\% in Valencia. While it is not possible to link actual transactions to idealista listings, this gives a quantitative sense of the coverage of the data package.

```{=tex}
\begin{table}[!ht]
\centering
\caption{Total properties and transactionst three Spanish cities. Year 2018} 
\label{tab:transactions} 
\begin{tabular}{lccccc}
City & Total properties (TP) & Total transactions (TT) & Listings (L) & L/TP & TT/L \\
\hline
Barcelona &  789,740  &  56,012  &  61,329  & 7.8\% & 81.3\%  \\ 
Madrid &  1,545,397  &  76,603  &  94,802  & 6.1\%  & 80.8\% \\ 
Valencia &  416,004  &  30,615  &  33,593  & 8.1\% & 91.1\% \\ 
\hline
Total &  2,751,141  &  163,230  &  189,724  & 6.9\% & 86.0\% \\ 
\hline
\multicolumn{6}{l}{{\footnotesize \textbf{Sources}}}\\
\multicolumn{6}{l}{{\footnotesize Total properties (P): Ministerio Español de Hacienda y Función Pública}}\\
\multicolumn{6}{l}{{\footnotesize Total transactions (T): Instituto Nacional de Estadística}}\\
\end{tabular}
\end{table}
```

---

>5. Given the asking price is a resized version of the original asking price with a random percentage between -2.5% and +2.5%, please elaborate on how this influences your quantitative research results and findings. Please provide critical evidence to explain how this randomised price will not influence academic research. For example, how does the house price variation at local level change after the price is randomised. To what degree will this influence a typical hedonic house price research approach? I am concerned about whether this randomised data could bias conclusions.

**Response:**

We appreciate the opportunity to delve more deeply into this issue, as it may be of interest to users of the data package. The short version of this story is that adding a small amount of random noise to the prices ($\pm 2.5\%$) does not introduce bias but does increase the variance of the masked variable. This can be shown with some standard algebra of random variables.

Now the longer story.

The masked prices are given by:
$$
P = RP\cdot \epsilon
$$

\noindent where $\epsilon$ is drawn from the uniform distribution with parameters $a=0.975$ and $b=1.025$:
$$
f(\epsilon) = 
\begin{cases}
\frac{1}{b - a} & \text{for } a\le \epsilon \le b\\
0 & \text{otherwise}
\end{cases}
$$

The expectation of $\epsilon$ given these parameters is:
$$
E[\epsilon] = \frac{a+b}{2} = \frac{0.975 + 1.025}{2} = 1
$$

\noindent and the variance of $\epsilon$ is:
$$
\mathrm{V}[\epsilon]=\frac{(b - a)^2}{12} = \frac{1}{4800}
$$

Therefore, the expectation of the masked prices is:
$$
\mathrm{E}[P] = \mathrm{E}[RP\cdot \epsilon] = \mathrm{E}[RP]\cdot \mathrm{E}[\epsilon] = \mathrm{E}[RP]
$$

In other words, the masked prices $P$ are an unbiased version of the raw prices $RP$.

Considering that $RP$ and $\epsilon$ are independent, the variance of the masked prices is as follows:
$$
\mathrm{V}[P] = \mathrm{V}[RP\cdot \epsilon] = \mathrm{V}[RP]\cdot \mathrm{V}[\epsilon] + \mathrm{V}[RP]\cdot (\mathrm{E}[\epsilon])^2 + \mathrm{V}[\epsilon]\cdot(\mathrm{E}[RP])^2
$$

Since $\mathrm{E}[\epsilon] = 1$, we have that:
$$
\mathrm{V}[P] = \mathrm{V}[RP]\cdot \mathrm{V}[\epsilon] + \mathrm{V}[RP] + \mathrm{V}[\epsilon]\cdot(\mathrm{E}[RP])^2 = \mathrm{V}[RP]\cdot (1 + \mathrm{V}[\epsilon]) + \mathrm{V}[\epsilon]\cdot(\mathrm{E}[RP])^2
$$

Solving for $\mathrm{V}[RP]$, and replacing the expectation of the raw prices by its unbiased version ($\mathrm{E}[P]$), yields the variance of the raw prices:
$$
\mathrm{V}[RP] = \frac{\mathrm{V}[P] - \frac{1}{4800}\cdot (\mathrm{E}[P])^2}{1 + \frac{1}{4800}}
$$

Table \ref{tab:variance} reports the mean and the standard deviation of the prices in the package, and the standard deviation of the raw prices calculated using the formula above. The last column of the table can be read as an inflation factor. We see that the variance of the masked prices is inflated with respect to the variance of the original values by less than $1\%$ in all cases examined. Users can use the formula above to calculate the inflation of the variance if they use subsamples other than those shown, to assess the potential impacts of the masking (e.g., when computing intervals of confidence).

```{r echo = FALSE}
data("Barcelona_Sale")
data("Madrid_Sale")
data("Valencia_Sale")
```

```{r echo=FALSE}
# Barcelona
Barcelona_Price <- Barcelona_Sale |>
  select(PRICE, PERIOD) |>
  st_drop_geometry()

BQ <- Barcelona_Price |>
  group_by(PERIOD) |>
  summarize(mean = mean(PRICE),
            variance = var(PRICE),
            .groups = "drop") |>
  mutate(sq_mean = mean^2,
         var_raw = (variance - sq_mean/4800)/(1 + 1/4800),
         inflation = variance/var_raw,
         CITY = "BARCELONA")

BT <- Barcelona_Price |>
  summarize(mean = mean(PRICE),
            variance = var(PRICE),
            .groups = "drop") |>
  mutate(PERIOD = "2018",
         sq_mean = mean^2,
         var_raw = (variance - sq_mean/4800)/(1 + 1/4800),
         inflation = variance/var_raw,
         CITY = "BARCELONA")

# Madrid
Madrid_Price <- Madrid_Sale |>
  select(PRICE, PERIOD) |>
  st_drop_geometry()

MQ <- Madrid_Price |>
  group_by(PERIOD) |>
  summarize(mean = mean(PRICE),
            variance = var(PRICE),
            .groups = "drop") |>
  mutate(sq_mean = mean^2,
         var_raw = (variance - sq_mean/4800)/(1 + 1/4800),
         inflation = variance/var_raw,
         CITY = "MADRID")

MT <- Madrid_Price |>
  summarize(mean = mean(PRICE),
            variance = var(PRICE),
            .groups = "drop") |>
  mutate(PERIOD = "2018",
         sq_mean = mean^2,
         var_raw = (variance - sq_mean/4800)/(1 + 1/4800),
         inflation = variance/var_raw,
         CITY = "MADRID")

# Valencia
Valencia_Price <- Valencia_Sale |>
  select(PRICE, PERIOD) |>
  st_drop_geometry()

VQ <- Valencia_Price |>
  group_by(PERIOD) |>
  summarize(mean = mean(PRICE),
            variance = var(PRICE),
            .groups = "drop") |>
  mutate(sq_mean = mean^2,
         var_raw = (variance - sq_mean/4800)/(1 + 1/4800),
         inflation = variance/var_raw,
         CITY = "VALENCIA")

VT <- Valencia_Price |>
  summarize(mean = mean(PRICE),
            variance = var(PRICE),
            .groups = "drop") |>
  mutate(PERIOD = "2018",
         sq_mean = mean^2,
         var_raw = (variance - sq_mean/4800)/(1 + 1/4800),
         inflation = variance/var_raw,
         CITY = "VALENCIA")

Q <- rbind(BQ,
           BT,
           MQ,
           MT,
           VQ,
           VT)

Q |>
  mutate(PERIOD = case_when(PERIOD == "201803" ~ "Q1",
                            PERIOD == "201806" ~ "Q2",
                            PERIOD == "201809" ~ "Q3",
                            PERIOD == "201812" ~ "Q4",
                            PERIOD == "2018" ~ "2018"),
         sd = round(sqrt(variance), 2),
         sd_raw = round(sqrt(var_raw), 2),
         inflation = round(inflation, 5),
         across(c(mean, sd, sd_raw, inflation), ~ prettyNum(.x, big.mark = ",")))  |>
  select(CITY, PERIOD, mean, sd, sd_raw, inflation) |>
  kableExtra::kable(booktabs = TRUE,
                    caption = "\\label{tab:variance}Inflation of the variance of masked prices with respect to raw prices",
                    col.names = c("City", "Period", "mean(P)", "sd(P)", "sd(RP)", "sd(P)/sd(RP)")) |>
  collapse_rows(columns = 1) |>
  footnote(general = c("P: masked prices; ", "RP: raw prices; ", "sd: standard deviation (square root of the variance)"),
           threeparttable = T)
```

---

>6. In figure 1, is it possible to standardise this data rather than showing only the number of dwelling records in your house price data? I guess the city centre always has high advertised volumes but it also has a high density of dwellings. It is worthwhile to show values standardised by the number of dwellings. 

**Response:** 

After this comment we included a new figure in the paper to display the percentage of listings relative to the total number of dwellings by district (see Figure \ref{fig:districts} below).

```{r, echo = FALSE,message=FALSE, fig.height=8, fig.cap="\\label{fig:districts}Percentage of listings relative to total number of dwellings by district. Boundary for Barcelona (Top), Madrid (Center), and Valencia (Bottom)."}
data("properties_by_district")

# Barcelona
Barcelona_prop <- properties_by_district |>
  filter(CITY == "Barcelona") |>
  mutate(proportion = N/N_CADASTRE)

q <- quantile(Barcelona_prop$proportion)
pq <- round(q * 100, 2)

Barcelona_prop <- Barcelona_prop |>
  mutate(Quantile = case_when(proportion <= q[2] ~ glue("[{pq[1]}%-{pq[2]}%)"),
                              proportion > q[2] & proportion <= q[3] ~ glue("[{pq[2]}%-{pq[3]}%)"),
                              proportion > q[3] & proportion <= q[4] ~ glue("[{pq[3]}%-{pq[4]}%)"),
                              proportion > q[4] ~ glue("[{pq[4]}%-{pq[5]}%)")))

plot4 <- ggplot(data = Barcelona_prop) +
  geom_sf(aes(fill = Quantile),
          color = NA) +
  geom_sf(data = Barcelona_Polygons,
          fill = NA) +
  theme_bw(base_size = 6) +
  scale_fill_manual(values=c("#FFFEDE",
                             "#FFDFA2",
                             "#FFA93F",
                             "#D5610D"))+
  theme(plot.margin=unit(c(0.1, 0.1, 0.1, 0.1),
                         "cm"),
        legend.position = "bottom",
        legend.title = element_blank())

# Madrid
Madrid_prop <- properties_by_district |>
  filter(CITY == "Madrid") |>
  mutate(proportion = N/N_CADASTRE)

q <- quantile(Madrid_prop$proportion)
pq <- round(q * 100, 2)

Madrid_prop <- Madrid_prop |>
  mutate(Quantile = case_when(proportion <= q[2] ~ glue("[{pq[1]}%-{pq[2]}%)"),
                              proportion > q[2] & proportion <= q[3] ~ glue("[{pq[2]}%-{pq[3]}%)"),
                              proportion > q[3] & proportion <= q[4] ~ glue("[{pq[3]}%-{pq[4]}%)"),
                              proportion > q[4] ~ glue("[{pq[4]}%-{pq[5]}%)")))


plot5 <- ggplot(data = Madrid_prop) +
  geom_sf(aes(fill = Quantile),
          color = NA,
          size=.2) +
  geom_sf(data = Madrid_Polygons,
          fill = NA) +
  theme_bw(base_size = 6) +
  scale_fill_manual(values=c("#FFFEDE",
                             "#FFDFA2",
                             "#FFA93F",
                             "#D5610D"))+
  theme(plot.margin=unit(c(0.1, 0.1, 0.1, 0.1),
                         "cm"),
        legend.position = "bottom",
        legend.title = element_blank())

# Valencia
Valencia_prop <- properties_by_district |>
  filter(CITY == "Valencia") |>
  mutate(proportion = N/N_CADASTRE)

q <- quantile(Valencia_prop$proportion)
pq <- round(q * 100, 2)

Valencia_prop <- Valencia_prop |>
  mutate(Quantile = case_when(proportion <= q[2] ~ glue("[{pq[1]}%-{pq[2]}%)"),
                              proportion > q[2] & proportion <= q[3] ~ glue("[{pq[2]}%-{pq[3]}%)"),
                              proportion > q[3] & proportion <= q[4] ~ glue("[{pq[3]}%-{pq[4]}%)"),
                              proportion > q[4] ~ glue("[{pq[4]}%-{pq[5]}%)")))


plot6 <- ggplot(data = Valencia_prop) +
  geom_sf(aes(fill = Quantile),
          color = NA,
          size=.2) +
  geom_sf(data = Valencia_Polygons,
          fill = NA) +
  theme_bw(base_size = 6) +
  scale_fill_manual(values=c("#FFFEDE",
                             "#FFDFA2",
                             "#FFA93F",
                             "#D5610D"))+
  theme(plot.margin=unit(c(0.1, 0.1, 0.1, 0.1),
                         "cm"),
        legend.position = "bottom",
        legend.title = element_blank())

# Arrange plots
grid.arrange(plot4, plot5, plot6, 
             ncol = 1, 
             nrow = 3)
```

---

Thank you again for your generous feedback. We hope that our responses will prove satisfactory and you will agree that the paper makes a worthwhile contribution to growing catalog of datasets to support research on real estate markets.

\newpage

## References
